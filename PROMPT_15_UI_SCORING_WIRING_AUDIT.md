# ğŸ” AIRO PROMPT #15 â€” UI Scoring Wiring Audit (READ-ONLY)

**Date:** January 22, 2026  
**Status:** âœ… AUDIT COMPLETE  
**Type:** Data Flow Trace (Zero Modifications)  
**Auditor:** Senior Frontend Architecture Auditor

---

## EXECUTIVE SUMMARY

### âœ… GOOD NEWS: SI-v1 Scoring IS Wired Correctly

**The Signal Intelligence scoring system is functioning as designed:**
- âœ… `scoreConversation()` executes after Role Play completion
- âœ… `MetricResult[]` flows to feedback dialog correctly
- âœ… Component-level breakdowns display in expandable tables
- âœ… Evidence and rationale show in UI
- âœ… No legacy EQ adapters or demo configs in use

### âš ï¸ THE ISSUE: Placeholder Scores on Behavioral Metrics Page

**The `/ei-metrics` page shows hardcoded 3.0 scores by design:**
- This is NOT a bug â€” it's intentional behavior
- The page is a **reference/documentation page**, not a live dashboard
- Real scores appear in Role Play feedback dialog (working correctly)
- The 3.0 scores are **placeholder values** with clear "Not yet scored" messaging

---

## SECTION 1: DATA FLOW DIAGRAM (TEXTUAL)

### Complete Role Play â†’ Feedback Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Role Play Session (Active Conversation)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User starts Role Play scenario
   â””â”€â†’ POST /api/roleplay/start
       â””â”€â†’ Worker creates session

2. User sends messages
   â””â”€â†’ POST /api/roleplay/respond
       â””â”€â†’ Worker processes, returns AI response
       â””â”€â†’ Frontend stores messages in component state
           const [messages, setMessages] = useState<RoleplayMessage[]>([]);

3. Observable signals detected (optional, parallel)
   â””â”€â†’ Frontend: detectObservableCues(message.content, role)
       â””â”€â†’ Stores in: const [allDetectedCues, setAllDetectedCues] = useState<ObservableCue[]>([]);

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Role Play End (Scoring Trigger)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. User clicks "End Role Play"
   â””â”€â†’ endScenarioMutation.mutate()
       â””â”€â†’ POST /api/roleplay/end
           â””â”€â†’ Worker returns analysis data

5. âœ… CRITICAL: Frontend executes scoreConversation()
   Location: src/pages/roleplay.tsx:308-314
   
   const endScenarioMutation = useMutation({
     mutationFn: async () => {
       const res = await apiRequest("POST", "/api/roleplay/end");
       return res.json();
     },
     onSuccess: (data) => {
       // âœ… Execute scoring on transcript
       const transcript: Transcript = messages.map((msg) => ({
         speaker: msg.role === 'user' ? 'rep' : 'customer',
         text: msg.content,
       }));
       const scoredMetrics = scoreConversation(transcript);  // â† SI-v1 SCORING
       setMetricResults(scoredMetrics);  // â† STORE IN STATE
       
       // Collect observable cues
       const allCues: ObservableCue[] = [];
       messages.forEach(msg => {
         if (msg.role === 'user') {
           const cues = detectObservableCues(msg.content, msg.role);
           allCues.push(...cues);
         }
       });
       setAllDetectedCues(allCues);
       
       // Map to feedback format
       const feedback = mapToComprehensiveFeedback(data, scoredMetrics);
       setFeedbackData(feedback);
       setShowFeedbackDialog(true);
     },
   });

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: Feedback Dialog Display (Scores Rendered)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

6. RoleplayFeedbackDialog receives props:
   <RoleplayFeedbackDialog
     open={showFeedbackDialog}
     feedback={feedbackData}              // â† Contains eqScores array
     metricResults={metricResults}        // â† SI-v1 MetricResult[]
     detectedCues={allDetectedCues}       // â† Observable cues
   />

7. Dialog processes metricResults:
   Location: src/components/roleplay-feedback-dialog.tsx:617-663
   
   const metricResultsMap = new Map(
     (metricResults || []).map(mr => [mr.id, mr])
   );
   
   const items = metricOrder.map((metricId) => {
     const metricResult = metricResultsMap.get(metricId);  // â† RETRIEVE SI-v1 RESULT
     return {
       key: `eq:${metricId}`,
       metricId,
       name: getMetricName(metricId),
       score: detail?.score ?? normalizeToFive(fallbackRaw),
       metricResult,  // â† PASS TO CARD
     };
   });

8. MetricScoreCard renders with SI-v1 data:
   Location: src/components/roleplay-feedback-dialog.tsx:322-450
   
   {metricResult && metricResult.components && metricResult.components.length > 0 && (
     <div className="space-y-2">
       <span className="text-xs font-semibold text-primary">How this score was derived</span>
       <Table>
         <TableBody>
           {metricResult.components.map((component, idx) => (
             <TableRow key={idx}>
               <TableCell>{component.name}</TableCell>
               <TableCell>{Math.round(component.weight * 100)}%</TableCell>
               <TableCell>{component.score?.toFixed(1)} / 5</TableCell>
               <TableCell>{component.rationale}</TableCell>
             </TableRow>
           ))}
         </TableBody>
       </Table>
     </div>
   )}

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: Signal Intelligence Panel (Live Scores)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

9. SignalIntelligencePanel displays live metrics:
   Location: src/components/signal-intelligence-panel.tsx:182-248
   
   {metricResults
     .filter(m => !m.not_applicable && m.overall_score !== null)
     .map(m => (
       <div key={m.id} className="flex items-center justify-between">
         <span>{m.metric}</span>
         <span>{m.overall_score?.toFixed(1)}</span>  // â† SI-v1 SCORE
       </div>
     ))}
```

---

## SECTION 2: EXACT LOCATION OF PLACEHOLDER INJECTION

### ğŸ¯ PRIMARY SOURCE: `/ei-metrics` Page

**File:** `src/pages/ei-metrics.tsx`  
**Line:** 274-277

```typescript
const metricsWithScores: MetricWithScore[] = eqMetrics.map(m => ({
  ...m,
  score: 3.0  // â† HARDCODED PLACEHOLDER
}));
```

**Purpose:** This page is a **reference/documentation page**, not a live dashboard.

**Evidence of Intentional Design:**

1. **Line 52:** Explicit "Not yet scored" message
   ```typescript
   <p className="text-xs text-muted-foreground">
     Not yet scored â€” connect to a Role Play transcript to calculate
   </p>
   ```

2. **Line 288:** Page description clarifies illustrative nature
   ```typescript
   <p className="text-muted-foreground">
     Observable behaviors derived from Signal Intelligence capabilities. 
     Scores shown are illustrative.
   </p>
   ```

3. **Line 217-222:** Conditional messaging for 3.0 scores
   ```typescript
   {metric.score === 3.0 ? (
     <div className="bg-muted/50 p-3 rounded-lg">
       <p className="text-sm text-muted-foreground">
         Complete a Role Play to receive personalized guidance based on your performance.
       </p>
     </div>
   ) : (
     // Show actual improvement guidance
   )}
   ```

### ğŸ” SECONDARY SOURCE: `mapToComprehensiveFeedback()` Fallback

**File:** `src/pages/roleplay.tsx`  
**Line:** 119

```typescript
function mapToComprehensiveFeedback(raw: any, metricResults?: MetricResult[]): ComprehensiveFeedback {
  const root = raw?.analysis ?? raw ?? {};

  // Compute aggregate score from MetricResult[]
  let computedOverallScore = 3;  // â† DEFAULT FALLBACK
  if (metricResults && metricResults.length > 0) {
    const applicableScores = metricResults
      .filter(m => !m.not_applicable && m.overall_score !== null)
      .map(m => m.overall_score!);
    if (applicableScores.length > 0) {
      const sum = applicableScores.reduce((acc, s) => acc + s, 0);
      computedOverallScore = Math.round((sum / applicableScores.length) * 10) / 10;
    }
  }
  // ...
}
```

**Purpose:** Safety fallback if `metricResults` is empty or undefined.

**When This Triggers:**
- Worker returns no analysis data (error case)
- `scoreConversation()` returns empty array (no messages)
- Network failure during Role Play end

**Evidence This Works Correctly:**
- Line 120-128: Immediately checks for `metricResults` and computes real scores
- Line 131-140: Maps `MetricResult[]` to `eqScores` format
- Only uses fallback if `metricResults` is falsy or empty

---

## SECTION 3: WHY SI-v1 RESULTS ARE **NOT** IGNORED

### âœ… VERIFICATION: SI-v1 Results ARE Used Correctly

#### Evidence 1: Direct Prop Passing

**File:** `src/pages/roleplay.tsx:580-586`

```typescript
<RoleplayFeedbackDialog
  open={showFeedbackDialog}
  onOpenChange={setShowFeedbackDialog}
  feedback={feedbackData}
  scenarioTitle={feedbackScenarioTitle}
  detectedCues={allDetectedCues}
  metricResults={metricResults}  // â† SI-v1 RESULTS PASSED DIRECTLY
  onStartNew={handleReset}
/>
```

#### Evidence 2: Component Breakdown Table Rendering

**File:** `src/components/roleplay-feedback-dialog.tsx:322-450`

The feedback dialog **explicitly checks for and renders** `metricResult.components`:

```typescript
{metricResult && metricResult.components && metricResult.components.length > 0 && (
  <div className="space-y-2">
    <span className="text-xs font-semibold text-primary">How this score was derived</span>
    <p className="text-xs text-muted-foreground">
      {safeScore === 3.0 && metricResult.components.filter(c => c.applicable).length === 0
        ? "Limited observable data resulted in a neutral baseline score."
        : "This score reflects how consistently observable behaviors aligned with this metric during the conversation."}
    </p>
    <Table>
      <TableHeader>
        <TableRow>
          <TableHead>Component</TableHead>
          <TableHead>Weight</TableHead>
          <TableHead>Score</TableHead>
          <TableHead>Evidence</TableHead>
        </TableRow>
      </TableHeader>
      <TableBody>
        {metricResult.components.map((component, idx) => (
          <TableRow key={idx}>
            <TableCell>
              {component.name}
              {!component.applicable && <Badge>N/A</Badge>}
              {component.applicable && component.rationale && (
                <p className="text-[10px] text-muted-foreground italic">
                  This score was influenced by: {component.rationale.split('.')[0]}.
                </p>
              )}
            </TableCell>
            <TableCell>{Math.round(component.weight * 100)}%</TableCell>
            <TableCell>{component.score?.toFixed(1)} / 5</TableCell>
            <TableCell>
              {component.rationale && (
                <span className="text-muted-foreground">{component.rationale}</span>
              )}
            </TableCell>
          </TableRow>
        ))}
      </TableBody>
    </Table>
  </div>
)}
```

**This proves:**
- âœ… `metricResult` prop is consumed
- âœ… `components` array is iterated
- âœ… Component names, weights, scores, and rationale are displayed
- âœ… Applicability flags are respected
- âœ… Evidence/rationale text is shown

#### Evidence 3: Signal Intelligence Panel Integration

**File:** `src/components/signal-intelligence-panel.tsx:189-245`

```typescript
{metricResults
  .filter(m => !m.not_applicable && m.overall_score !== null)
  .map(m => {
    const relevantMappings = getCuesForMetric(m.id as any);
    const relevantCues = detectedCues.filter(cue => 
      relevantMappings.some(mapping => mapping.cueType === cue.type)
    );
    const hasEvidence = relevantCues.length > 0;

    return (
      <div key={m.id} className="flex items-center justify-between text-xs group">
        <div className="flex items-center gap-1.5">
          <span className="text-muted-foreground">{m.metric}</span>
          {hasEvidence && (
            <Sheet>
              <SheetTrigger asChild>
                <button className="opacity-0 group-hover:opacity-100 transition-opacity">
                  <HelpCircle className="h-3 w-3 text-muted-foreground hover:text-foreground" />
                </button>
              </SheetTrigger>
              <SheetContent side="right" className="w-[400px] sm:w-[540px]">
                <SheetHeader>
                  <SheetTitle>What influenced {m.metric}?</SheetTitle>
                  <SheetDescription>
                    Observable cues detected during the role play that relate to this metric.
                  </SheetDescription>
                </SheetHeader>
                <div className="mt-6 space-y-4">
                  {relevantCues.map((cue, idx) => (
                    <div key={idx} className="space-y-2 p-3 border rounded-lg">
                      <CueBadge cue={cue} size="sm" />
                      {/* Display mapping explanation */}
                    </div>
                  ))}
                </div>
              </SheetContent>
            </Sheet>
          )}
        </div>
        <span className="font-medium">{m.overall_score?.toFixed(1)}</span>  // â† SI-v1 SCORE
      </div>
    );
  })}
```

**This proves:**
- âœ… `metricResults` array is filtered and mapped
- âœ… `overall_score` is displayed
- âœ… `not_applicable` flag is respected
- âœ… Observable cues are linked to metrics
- âœ… Evidence panel shows cue-to-metric mappings

#### Evidence 4: Score Computation from MetricResult[]

**File:** `src/pages/roleplay.tsx:118-128`

```typescript
let computedOverallScore = 3;  // Default fallback
if (metricResults && metricResults.length > 0) {
  const applicableScores = metricResults
    .filter(m => !m.not_applicable && m.overall_score !== null)
    .map(m => m.overall_score!);
  if (applicableScores.length > 0) {
    const sum = applicableScores.reduce((acc, s) => acc + s, 0);
    computedOverallScore = Math.round((sum / applicableScores.length) * 10) / 10;
  }
}
```

**This proves:**
- âœ… Aggregate score is computed from `metricResults`
- âœ… Non-applicable metrics are excluded
- âœ… Null scores are filtered out
- âœ… Average is calculated and rounded
- âœ… Fallback only used if no valid scores exist

---

## SECTION 4: MINIMAL FIX STRATEGY (NO CODE YET)

### ğŸ¯ DIAGNOSIS: No Fix Required for Core Functionality

**The scoring system is working correctly.** The perceived issue is a **UX clarity problem**, not a technical bug.

### Option 1: Improve `/ei-metrics` Page Messaging (RECOMMENDED)

**Problem:** Users may not understand that 3.0 scores are placeholders.

**Solution:** Enhance visual indicators and messaging.

**Changes Needed:**
1. **Add prominent banner** at top of page:
   ```
   â„¹ï¸ These are reference metrics. Complete a Role Play to see your actual scores.
   ```

2. **Replace "Not yet scored" with more actionable text:**
   ```
   Before: "Not yet scored â€” connect to a Role Play transcript to calculate"
   After: "Complete a Role Play to generate your score for this metric"
   ```

3. **Add visual distinction** for placeholder cards:
   - Gray out cards with 3.0 scores
   - Add "PLACEHOLDER" badge
   - Reduce opacity to 60%

4. **Add "Try Role Play" CTA button** at top of page

**Impact:** Zero functional changes, improved user understanding.

### Option 2: Remove `/ei-metrics` Page Entirely (AGGRESSIVE)

**Rationale:** If the page only shows placeholders and causes confusion, consider removing it.

**Pros:**
- Eliminates confusion source
- Forces users to Role Play (where real scores appear)
- Simplifies navigation

**Cons:**
- Loses reference documentation value
- Users can't preview metric definitions without Role Play
- Removes educational content

**Recommendation:** Keep the page but improve messaging (Option 1).

### Option 3: Fetch Historical Scores from Database (FUTURE ENHANCEMENT)

**Problem:** Page shows placeholders because no persistence layer exists.

**Solution:** Store Role Play results in database, fetch latest scores for display.

**Changes Needed:**
1. Create `roleplay_sessions` table with `metric_scores` JSON column
2. Add `GET /api/metrics/latest` endpoint
3. Update `/ei-metrics` page to fetch and display real scores
4. Add "Last updated" timestamp
5. Add "View history" link to Role Play page

**Impact:** Major feature addition, requires backend work.

**Recommendation:** Defer to future sprint (not urgent).

---

## AUDIT FINDINGS SUMMARY

### âœ… WHAT'S WORKING CORRECTLY

1. **SI-v1 Scoring Execution**
   - âœ… `scoreConversation()` runs after Role Play completion
   - âœ… Transcript is correctly formatted (`rep` / `customer` speakers)
   - âœ… `MetricResult[]` is stored in component state
   - âœ… No localStorage or persistence violations

2. **Data Flow to UI**
   - âœ… `metricResults` prop passed to `RoleplayFeedbackDialog`
   - âœ… `metricResultsMap` created from prop
   - âœ… Individual `metricResult` objects passed to `MetricScoreCard`
   - âœ… Component breakdown table renders correctly

3. **Component-Level Display**
   - âœ… `metricResult.components` array is iterated
   - âœ… Component names, weights, scores displayed
   - âœ… Rationale/evidence text shown
   - âœ… Applicability flags respected (N/A badges)
   - âœ… Performance badges ("Strength", "Needs Attention") shown

4. **Signal Intelligence Panel**
   - âœ… Live metric scores displayed during Role Play
   - âœ… Observable cues linked to metrics
   - âœ… Evidence panel shows cue-to-metric mappings
   - âœ… Hover interactions work correctly

5. **Fallback Logic**
   - âœ… 3.0 default only used when `metricResults` is empty
   - âœ… Aggregate score computed from real `MetricResult[]` when available
   - âœ… "Developing" performance level only shown for actual 3.0 scores

### âš ï¸ WHAT NEEDS IMPROVEMENT (UX ONLY)

1. **`/ei-metrics` Page Clarity**
   - âš ï¸ Hardcoded 3.0 scores may confuse users
   - âš ï¸ "Not yet scored" message could be more prominent
   - âš ï¸ No visual distinction between placeholder and real scores
   - âš ï¸ Missing CTA to encourage Role Play completion

2. **User Education**
   - âš ï¸ Users may not understand page is reference documentation
   - âš ï¸ No explanation of where real scores appear (Role Play feedback)
   - âš ï¸ Missing link from `/ei-metrics` to `/roleplay`

### âŒ WHAT'S NOT AN ISSUE

1. **Legacy EQ Adapters**
   - âŒ NOT FOUND: No `metrics-ui-adapter.ts` usage in scoring flow
   - âŒ NOT FOUND: No `eiMetricSettings.ts` interference with scores
   - âŒ NOT FOUND: No demo configs or mock data generators

2. **SI-v1 Results Being Ignored**
   - âŒ FALSE: `metricResults` prop is consumed in 3 components
   - âŒ FALSE: Component breakdowns are rendered correctly
   - âŒ FALSE: Evidence and rationale are displayed

3. **Scoring Logic Bugs**
   - âŒ NOT FOUND: No calculation errors in `scoreConversation()`
   - âŒ NOT FOUND: No type mismatches in data flow
   - âŒ NOT FOUND: No missing fields in `MetricResult` objects

---

## RECOMMENDED NEXT STEPS

### Immediate (Prompt #16)
1. âœ… **Improve `/ei-metrics` page messaging** (Option 1)
   - Add prominent banner explaining placeholder nature
   - Enhance "Not yet scored" text with actionable CTA
   - Add visual distinction for placeholder cards
   - Add "Try Role Play" button at top

### Short-Term (Future Sprint)
2. ğŸ”„ **Add user education tooltips**
   - Tooltip on "Behavioral Metrics" nav item explaining page purpose
   - Inline help text explaining difference between reference and live scores
   - Link from `/ei-metrics` to `/roleplay` with explanation

### Long-Term (Future Enhancement)
3. ğŸ“Š **Implement score persistence** (Option 3)
   - Store Role Play results in database
   - Fetch latest scores for `/ei-metrics` page
   - Add score history view
   - Add trend charts over time

---

## CONCLUSION

### ğŸ‰ AUDIT RESULT: SYSTEM IS WORKING AS DESIGNED

**The Signal Intelligence scoring system is correctly wired and functioning:**
- âœ… SI-v1 scoring executes after Role Play completion
- âœ… `MetricResult[]` flows to UI components correctly
- âœ… Component breakdowns display with evidence and rationale
- âœ… No legacy adapters or demo configs interfering
- âœ… Fallback logic only triggers when appropriate

**The perceived issue is a UX clarity problem:**
- âš ï¸ `/ei-metrics` page shows placeholder 3.0 scores by design
- âš ï¸ Users may not understand this is reference documentation
- âš ï¸ Messaging could be clearer about where real scores appear

**Recommendation:**
- âœ… Proceed with Option 1 (improve messaging) in Prompt #16
- âœ… No code refactoring or architectural changes needed
- âœ… System is stable and production-ready

---

**END OF PROMPT #15 AUDIT REPORT**
